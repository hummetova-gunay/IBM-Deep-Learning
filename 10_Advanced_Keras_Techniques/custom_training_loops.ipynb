{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16c61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f867bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee31506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghummatova\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define a model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape = (28,28)),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b557e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.552339553833008\n",
      "Epoch 1 Step 200: Loss = 0.4023306965827942\n",
      "Epoch 1 Step 400: Loss = 0.18397483229637146\n",
      "Epoch 1 Step 600: Loss = 0.18389032781124115\n",
      "Epoch 1 Step 800: Loss = 0.1440712809562683\n",
      "Epoch 1 Step 1000: Loss = 0.4658580422401428\n",
      "Epoch 1 Step 1200: Loss = 0.18269610404968262\n",
      "Epoch 1 Step 1400: Loss = 0.23084281384944916\n",
      "Epoch 1 Step 1600: Loss = 0.20811232924461365\n",
      "Epoch 1 Step 1800: Loss = 0.12597188353538513\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.08379092812538147\n",
      "Epoch 2 Step 200: Loss = 0.14957763254642487\n",
      "Epoch 2 Step 400: Loss = 0.11971994489431381\n",
      "Epoch 2 Step 600: Loss = 0.071737140417099\n",
      "Epoch 2 Step 800: Loss = 0.06520923972129822\n",
      "Epoch 2 Step 1000: Loss = 0.24964426457881927\n",
      "Epoch 2 Step 1200: Loss = 0.10491427034139633\n",
      "Epoch 2 Step 1400: Loss = 0.1583489179611206\n",
      "Epoch 2 Step 1600: Loss = 0.1695874035358429\n",
      "Epoch 2 Step 1800: Loss = 0.06552714854478836\n"
     ]
    }
   ],
   "source": [
    "# implement the Custom Training Loop\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cdc76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance the custom training loop by adding an accuracy metric to monitor model performance\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c367285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 0.04134659469127655 Accuracy = 1.0\n",
      "Epoch 1 Step 200: Loss = 0.05511271208524704 Accuracy = 0.9735696315765381\n",
      "Epoch 1 Step 400: Loss = 0.10105249285697937 Accuracy = 0.9724127054214478\n",
      "Epoch 1 Step 600: Loss = 0.04928106814622879 Accuracy = 0.9734296798706055\n",
      "Epoch 1 Step 800: Loss = 0.03739263489842415 Accuracy = 0.973665714263916\n",
      "Epoch 1 Step 1000: Loss = 0.11521053314208984 Accuracy = 0.9741820693016052\n",
      "Epoch 1 Step 1200: Loss = 0.04165233299136162 Accuracy = 0.9746044874191284\n",
      "Epoch 1 Step 1400: Loss = 0.1316077560186386 Accuracy = 0.9750401377677917\n",
      "Epoch 1 Step 1600: Loss = 0.12712788581848145 Accuracy = 0.9746837615966797\n",
      "Epoch 1 Step 1800: Loss = 0.0488007627427578 Accuracy = 0.9751526713371277\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.016839366406202316 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.06743358075618744 Accuracy = 0.981809675693512\n",
      "Epoch 2 Step 400: Loss = 0.07800669968128204 Accuracy = 0.9810629487037659\n",
      "Epoch 2 Step 600: Loss = 0.04794783890247345 Accuracy = 0.9823731184005737\n",
      "Epoch 2 Step 800: Loss = 0.02788996510207653 Accuracy = 0.9824438095092773\n",
      "Epoch 2 Step 1000: Loss = 0.12254991382360458 Accuracy = 0.9826423525810242\n",
      "Epoch 2 Step 1200: Loss = 0.018114052712917328 Accuracy = 0.9826186299324036\n",
      "Epoch 2 Step 1400: Loss = 0.08243298530578613 Accuracy = 0.9828247427940369\n",
      "Epoch 2 Step 1600: Loss = 0.05893964692950249 Accuracy = 0.9824718832969666\n",
      "Epoch 2 Step 1800: Loss = 0.03346630558371544 Accuracy = 0.982822060585022\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.02045302651822567 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.036697182804346085 Accuracy = 0.9874067306518555\n",
      "Epoch 3 Step 400: Loss = 0.0912071019411087 Accuracy = 0.9868298172950745\n",
      "Epoch 3 Step 600: Loss = 0.026370801031589508 Accuracy = 0.9876767992973328\n",
      "Epoch 3 Step 800: Loss = 0.03544621169567108 Accuracy = 0.9877106547355652\n",
      "Epoch 3 Step 1000: Loss = 0.07925643771886826 Accuracy = 0.9881680607795715\n",
      "Epoch 3 Step 1200: Loss = 0.012757990509271622 Accuracy = 0.9882129430770874\n",
      "Epoch 3 Step 1400: Loss = 0.040793366730213165 Accuracy = 0.9879773259162903\n",
      "Epoch 3 Step 1600: Loss = 0.04530057683587074 Accuracy = 0.9878200888633728\n",
      "Epoch 3 Step 1800: Loss = 0.023763027042150497 Accuracy = 0.9879407286643982\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.0160566084086895 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.03569163382053375 Accuracy = 0.990516185760498\n",
      "Epoch 4 Step 400: Loss = 0.10399017482995987 Accuracy = 0.990414559841156\n",
      "Epoch 4 Step 600: Loss = 0.021638693287968636 Accuracy = 0.9906926155090332\n",
      "Epoch 4 Step 800: Loss = 0.04903624206781387 Accuracy = 0.9910268187522888\n",
      "Epoch 4 Step 1000: Loss = 0.07437142729759216 Accuracy = 0.9916333556175232\n",
      "Epoch 4 Step 1200: Loss = 0.008313704282045364 Accuracy = 0.9917776584625244\n",
      "Epoch 4 Step 1400: Loss = 0.025593694299459457 Accuracy = 0.9914792776107788\n",
      "Epoch 4 Step 1600: Loss = 0.03172896429896355 Accuracy = 0.9912945032119751\n",
      "Epoch 4 Step 1800: Loss = 0.01660151034593582 Accuracy = 0.9912028312683105\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.011137419380247593 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.021787628531455994 Accuracy = 0.9931591749191284\n",
      "Epoch 5 Step 400: Loss = 0.11511935293674469 Accuracy = 0.9931421279907227\n",
      "Epoch 5 Step 600: Loss = 0.03173132985830307 Accuracy = 0.9938123822212219\n",
      "Epoch 5 Step 800: Loss = 0.053783826529979706 Accuracy = 0.9941089153289795\n",
      "Epoch 5 Step 1000: Loss = 0.03331032022833824 Accuracy = 0.9944430589675903\n",
      "Epoch 5 Step 1200: Loss = 0.01085265539586544 Accuracy = 0.9945618510246277\n",
      "Epoch 5 Step 1400: Loss = 0.021147901192307472 Accuracy = 0.9943343997001648\n",
      "Epoch 5 Step 1600: Loss = 0.021676499396562576 Accuracy = 0.9941638112068176\n",
      "Epoch 5 Step 1800: Loss = 0.008713111281394958 Accuracy = 0.9939616918563843\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce6f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback for advanced logging\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
