{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d16c61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f867bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set Up the Environment\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee31506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gunay\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define a model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape = (28,28)),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9a0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b557e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 2.349669933319092\n",
      "Epoch 1 Step 200: Loss = 0.38923656940460205\n",
      "Epoch 1 Step 400: Loss = 0.18076735734939575\n",
      "Epoch 1 Step 600: Loss = 0.19310754537582397\n",
      "Epoch 1 Step 800: Loss = 0.1458130031824112\n",
      "Epoch 1 Step 1000: Loss = 0.4679909348487854\n",
      "Epoch 1 Step 1200: Loss = 0.1573670506477356\n",
      "Epoch 1 Step 1400: Loss = 0.2595573663711548\n",
      "Epoch 1 Step 1600: Loss = 0.19441808760166168\n",
      "Epoch 1 Step 1800: Loss = 0.20664526522159576\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.07567359507083893\n",
      "Epoch 2 Step 200: Loss = 0.11164087057113647\n",
      "Epoch 2 Step 400: Loss = 0.11896685510873795\n",
      "Epoch 2 Step 600: Loss = 0.05577671527862549\n",
      "Epoch 2 Step 800: Loss = 0.09995268285274506\n",
      "Epoch 2 Step 1000: Loss = 0.26084694266319275\n",
      "Epoch 2 Step 1200: Loss = 0.09275107830762863\n",
      "Epoch 2 Step 1400: Loss = 0.196408212184906\n",
      "Epoch 2 Step 1600: Loss = 0.17089885473251343\n",
      "Epoch 2 Step 1800: Loss = 0.14739060401916504\n"
     ]
    }
   ],
   "source": [
    "# implement the Custom Training Loop\n",
    "\n",
    "epochs = 2\n",
    "# train_dataset = train_dataset.repeat(epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Logging the loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cdc76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance the custom training loop by adding an accuracy metric to monitor model performance\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function for multi-class classification\n",
    "optimizer = tf.keras.optimizers.Adam()  # Adam optimizer for efficient training\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c367285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 0.034793607890605927 Accuracy = 1.0\n",
      "Epoch 1 Step 200: Loss = 0.03796670585870743 Accuracy = 0.9749689102172852\n",
      "Epoch 1 Step 400: Loss = 0.12114369124174118 Accuracy = 0.9721789360046387\n",
      "Epoch 1 Step 600: Loss = 0.024428755044937134 Accuracy = 0.9737417101860046\n",
      "Epoch 1 Step 800: Loss = 0.06340333819389343 Accuracy = 0.9740168452262878\n",
      "Epoch 1 Step 1000: Loss = 0.16261179745197296 Accuracy = 0.9745879173278809\n",
      "Epoch 1 Step 1200: Loss = 0.07859534025192261 Accuracy = 0.9750728607177734\n",
      "Epoch 1 Step 1400: Loss = 0.10269129276275635 Accuracy = 0.9755085706710815\n",
      "Epoch 1 Step 1600: Loss = 0.12958714365959167 Accuracy = 0.9750156402587891\n",
      "Epoch 1 Step 1800: Loss = 0.08711864799261093 Accuracy = 0.9754476547241211\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.015432622283697128 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.015945684164762497 Accuracy = 0.9811878204345703\n",
      "Epoch 2 Step 400: Loss = 0.08605445176362991 Accuracy = 0.9806733131408691\n",
      "Epoch 2 Step 600: Loss = 0.013486292213201523 Accuracy = 0.9817491769790649\n",
      "Epoch 2 Step 800: Loss = 0.04196038097143173 Accuracy = 0.9820146560668945\n",
      "Epoch 2 Step 1000: Loss = 0.14741834998130798 Accuracy = 0.982361376285553\n",
      "Epoch 2 Step 1200: Loss = 0.047213125973939896 Accuracy = 0.9824365377426147\n",
      "Epoch 2 Step 1400: Loss = 0.06392814218997955 Accuracy = 0.9826239943504333\n",
      "Epoch 2 Step 1600: Loss = 0.08367514610290527 Accuracy = 0.9825109243392944\n",
      "Epoch 2 Step 1800: Loss = 0.0673975795507431 Accuracy = 0.9826138019561768\n",
      "Start of epoch 3\n",
      "Epoch 3 Step 0: Loss = 0.012085724622011185 Accuracy = 1.0\n",
      "Epoch 3 Step 200: Loss = 0.010357637889683247 Accuracy = 0.986784815788269\n",
      "Epoch 3 Step 400: Loss = 0.06899277120828629 Accuracy = 0.9869856834411621\n",
      "Epoch 3 Step 600: Loss = 0.017750591039657593 Accuracy = 0.9877808094024658\n",
      "Epoch 3 Step 800: Loss = 0.038772743195295334 Accuracy = 0.9877886772155762\n",
      "Epoch 3 Step 1000: Loss = 0.12842519581317902 Accuracy = 0.9878559112548828\n",
      "Epoch 3 Step 1200: Loss = 0.030043719336390495 Accuracy = 0.9876925349235535\n",
      "Epoch 3 Step 1400: Loss = 0.03181088715791702 Accuracy = 0.9879773259162903\n",
      "Epoch 3 Step 1600: Loss = 0.062288444489240646 Accuracy = 0.9878981709480286\n",
      "Epoch 3 Step 1800: Loss = 0.03037632629275322 Accuracy = 0.9879580736160278\n",
      "Start of epoch 4\n",
      "Epoch 4 Step 0: Loss = 0.011480691842734814 Accuracy = 1.0\n",
      "Epoch 4 Step 200: Loss = 0.014159200713038445 Accuracy = 0.9912935495376587\n",
      "Epoch 4 Step 400: Loss = 0.05637267231941223 Accuracy = 0.9902587532997131\n",
      "Epoch 4 Step 600: Loss = 0.021053463220596313 Accuracy = 0.9909525513648987\n",
      "Epoch 4 Step 800: Loss = 0.012015795335173607 Accuracy = 0.9907147288322449\n",
      "Epoch 4 Step 1000: Loss = 0.12331076711416245 Accuracy = 0.9911338686943054\n",
      "Epoch 4 Step 1200: Loss = 0.01558130607008934 Accuracy = 0.9913093447685242\n",
      "Epoch 4 Step 1400: Loss = 0.019375357776880264 Accuracy = 0.9914124011993408\n",
      "Epoch 4 Step 1600: Loss = 0.05779224634170532 Accuracy = 0.9912945032119751\n",
      "Epoch 4 Step 1800: Loss = 0.03117748349905014 Accuracy = 0.991324245929718\n",
      "Start of epoch 5\n",
      "Epoch 5 Step 0: Loss = 0.009655039757490158 Accuracy = 1.0\n",
      "Epoch 5 Step 200: Loss = 0.00725668715313077 Accuracy = 0.9933146834373474\n",
      "Epoch 5 Step 400: Loss = 0.027244582772254944 Accuracy = 0.9929083585739136\n",
      "Epoch 5 Step 600: Loss = 0.017821811139583588 Accuracy = 0.9936563968658447\n",
      "Epoch 5 Step 800: Loss = 0.010434040799736977 Accuracy = 0.9936797618865967\n",
      "Epoch 5 Step 1000: Loss = 0.09467286616563797 Accuracy = 0.9937874674797058\n",
      "Epoch 5 Step 1200: Loss = 0.010778418742120266 Accuracy = 0.9937812089920044\n",
      "Epoch 5 Step 1400: Loss = 0.01617511175572872 Accuracy = 0.9937098622322083\n",
      "Epoch 5 Step 1600: Loss = 0.02100120671093464 Accuracy = 0.9935977458953857\n",
      "Epoch 5 Step 1800: Loss = 0.013766743242740631 Accuracy = 0.9936667084693909\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement the Custom Training Loop with Accuracy\n",
    "\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce6f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback for advanced logging\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Step 4: Implement the Custom Callback \n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f'End of epoch {epoch + 1}, loss: {logs.get(\"loss\")}, accuracy: {logs.get(\"accuracy\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5aefdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Step 0: Loss = 0.009429750964045525 Accuracy = 1.0\n",
      "Epoch 1 Step 200: Loss = 0.013684853911399841 Accuracy = 0.9942474961280823\n",
      "Epoch 1 Step 400: Loss = 0.03636747971177101 Accuracy = 0.9945448637008667\n",
      "Epoch 1 Step 600: Loss = 0.03305281698703766 Accuracy = 0.9945923686027527\n",
      "Epoch 1 Step 800: Loss = 0.03783285990357399 Accuracy = 0.9946160912513733\n",
      "Epoch 1 Step 1000: Loss = 0.04316720366477966 Accuracy = 0.9945366978645325\n",
      "Epoch 1 Step 1200: Loss = 0.014143750071525574 Accuracy = 0.9945878386497498\n",
      "Epoch 1 Step 1400: Loss = 0.013795854523777962 Accuracy = 0.9945128560066223\n",
      "Epoch 1 Step 1600: Loss = 0.018518628552556038 Accuracy = 0.994573712348938\n",
      "Epoch 1 Step 1800: Loss = 0.012339422479271889 Accuracy = 0.9945690035820007\n",
      "End of epoch 1, loss: 0.002918898593634367, accuracy: 0.9946833252906799\n",
      "Start of epoch 2\n",
      "Epoch 2 Step 0: Loss = 0.016985910013318062 Accuracy = 1.0\n",
      "Epoch 2 Step 200: Loss = 0.0026502422988414764 Accuracy = 0.9937810897827148\n",
      "Epoch 2 Step 400: Loss = 0.022253356873989105 Accuracy = 0.9950124621391296\n",
      "Epoch 2 Step 600: Loss = 0.009583738632500172 Accuracy = 0.9957882761955261\n",
      "Epoch 2 Step 800: Loss = 0.004245103802531958 Accuracy = 0.9957475066184998\n",
      "Epoch 2 Step 1000: Loss = 0.033477503806352615 Accuracy = 0.9958478808403015\n",
      "Epoch 2 Step 1200: Loss = 0.009867392480373383 Accuracy = 0.9958367943763733\n",
      "Epoch 2 Step 1400: Loss = 0.0009482690948061645 Accuracy = 0.9959626793861389\n",
      "Epoch 2 Step 1600: Loss = 0.016251714900135994 Accuracy = 0.9960766434669495\n",
      "Epoch 2 Step 1800: Loss = 0.005844137631356716 Accuracy = 0.9960785508155823\n",
      "End of epoch 2, loss: 0.0029129351023584604, accuracy: 0.9961666464805603\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement the Custom Training Loop with Custom Callback\n",
    "\n",
    "epochs = 2\n",
    "custom_callback = CustomCallback()  # Initialize the custom callback\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch + 1}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # Compute loss\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Apply gradients to update model weights\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        accuracy_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log the loss and accuracy every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
    "    \n",
    "    # Call the custom callback at the end of each epoch\n",
    "    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})\n",
    "    \n",
    "    # Reset the metric at the end of each epoch\n",
    "    accuracy_metric.reset_state()  # Use reset_state() instead of reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f78433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "input_layer = Input(shape=(28, 28))  # Input layer with shape (28, 28)\n",
    "\n",
    "# Define hidden layers\n",
    "hidden_layer1 = Dense(64, activation='relu')(input_layer)  # First hidden layer with 64 neurons and ReLU activation\n",
    "hidden_layer2 = Dense(64, activation='relu')(hidden_layer1)  # Second hidden layer with 64 neurons and ReLU activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2918323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c62c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c4db41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d70f26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4890 - loss: 0.7022 \n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5110 - loss: 0.6940\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5210 - loss: 0.6913 \n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5280 - loss: 0.6903\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5650 - loss: 0.6873\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5570 - loss: 0.6866 \n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5650 - loss: 0.6838\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5870 - loss: 0.6824 \n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5470 - loss: 0.6832\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5860 - loss: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x214ffe05fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Redefine the Model for 20 features\n",
    "model = Sequential([\n",
    "    Input(shape=(20,)),  # Adjust input shape to (20,)\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification with sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 2: Generate Example Data\n",
    "X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features each\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # 1000 binary labels (0 or 1)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0afa374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5000 - loss: 0.6902 \n",
      "Test loss: 0.6901620626449585\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Example test data (in practice, use real dataset)\n",
    "X_test = np.random.rand(200, 20)  # 200 samples, 20 features each\n",
    "y_test = np.random.randint(2, size=(200, 1))  # 200 binary labels (0 or 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
