{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b4356e",
   "metadata": {},
   "source": [
    "# Classification and Captioning Aircraft Damage Using Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7a3cf",
   "metadata": {},
   "source": [
    "### Part 1. Classification Problem: Classifying the defect on the aircraft as 'dent' or 'crack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e08bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649f854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313b0be",
   "metadata": {},
   "source": [
    "#### 1.1 Dataset preparation\n",
    "After you complete the project, you will be able to:\n",
    "\n",
    "- Use the VGG16 model for image classification.\n",
    "- Prepare and preprocess image data for a machine learning task.\n",
    "- Evaluate the model’s performance using appropriate metrics.\n",
    "- Visualize model predictions on test data.\n",
    "- Use a custom Keras layer. \n",
    "\n",
    "\n",
    " ### Task List\n",
    "To achieve the above objectives, you will complete the following tasks:\n",
    "\n",
    "- Task 1: Create a `valid_generator` using the `valid_datagen` object\n",
    "- Task 2: Create a `test_generator` using the `test_datagen` object\n",
    "- Task 3: Load the VGG16 model\n",
    "- Task 4: Compile the model\n",
    "- Task 5: Train the model\n",
    "- Task 6: Plot accuracy curves for training and validation sets \n",
    "- Task 7: Visualizing the results \n",
    "- Task 8: Implement a Helper Function to Use the Custom Keras Layer\n",
    "- Task 9: Generate a caption for an image using the using BLIP pretrained model\n",
    "- Task 10: Generate a summary of an image using BLIP pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509e7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the batch size,epochs\n",
    "batch_size =32\n",
    "n_epochs = 5\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0e1d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded aircraft_damage_dataset_v1.tar. Extraction will begin now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunay\\AppData\\Local\\Temp\\ipykernel_32872\\2111111657.py:27: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar_ref.extractall()  # This will extract to the current directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted aircraft_damage_dataset_v1.tar successfully.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# URL of the tar file\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZjXM4RKxlBK9__ZjHBLl5A/aircraft-damage-dataset-v1.tar\"\n",
    "\n",
    "# Define the path to save the file\n",
    "tar_filename = \"aircraft_damage_dataset_v1.tar\"\n",
    "extracted_folder = \"aircraft_damage_dataset_v1\"  # Folder where contents will be extracted\n",
    "\n",
    "# Download the tar file\n",
    "urllib.request.urlretrieve(url, tar_filename)\n",
    "print(f\"Downloaded {tar_filename}. Extraction will begin now.\")\n",
    "\n",
    "# Check if the folder already exists\n",
    "if os.path.exists(extracted_folder):\n",
    "    print(f\"The folder '{extracted_folder}' already exists. Removing the existing folder.\")\n",
    "    \n",
    "    # Remove the existing folder to avoid overwriting or duplication\n",
    "    shutil.rmtree(extracted_folder)\n",
    "    print(f\"Removed the existing folder: {extracted_folder}\")\n",
    "\n",
    "# Extract the contents of the tar file\n",
    "with tarfile.open(tar_filename, \"r\") as tar_ref:\n",
    "    tar_ref.extractall()  # This will extract to the current directory\n",
    "    print(f\"Extracted {tar_filename} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80255534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for train, test, and validation splits\n",
    "extract_path = \"aircraft_damage_dataset_v1\"\n",
    "train_dir = os.path.join(extract_path, 'train')\n",
    "test_dir = os.path.join(extract_path, 'test')\n",
    "valid_dir = os.path.join(extract_path, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624caeef",
   "metadata": {},
   "source": [
    "#### 1.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf5a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ImageDataGenerators to preprocess the data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9a46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_rows, img_cols),   # Resize images to the size VGG16 expects\n",
    "    batch_size=batch_size,\n",
    "    seed = seed_value,\n",
    "    class_mode='binary',\n",
    "    shuffle=True # Binary classification: dent vs crack\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9c255",
   "metadata": {},
   "source": [
    "##### Task 1. Create a valid_generator using the valid_datagen object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4a6c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=valid_dir,\n",
    "    class_mode='binary',\n",
    "    seed=seed_value,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    target_size=(img_rows, img_cols)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45039425",
   "metadata": {},
   "source": [
    "##### Task 2. Create a test_generator using test_gen object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c198dfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    class_mode='binary',\n",
    "    seed=seed_value,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    target_size=(img_rows, img_cols)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c294151",
   "metadata": {},
   "source": [
    "##### Task 3. Load the pre-trained model VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4775a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(img_rows, img_cols, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce6afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = base_model.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "base_model = Model(base_model.input, output)\n",
    "\n",
    "# Freeze the base VGG16 model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041e3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the custom model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b156a89",
   "metadata": {},
   "source": [
    "##### Task 4. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3020c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357f985",
   "metadata": {},
   "source": [
    "##### Task 5. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f86799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 8s/step - accuracy: 0.5367 - loss: 0.7201 - val_accuracy: 0.6042 - val_loss: 0.6338\n",
      "Epoch 2/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 7s/step - accuracy: 0.7233 - loss: 0.5536 - val_accuracy: 0.6979 - val_loss: 0.5823\n",
      "Epoch 3/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 12s/step - accuracy: 0.7767 - loss: 0.4737 - val_accuracy: 0.7188 - val_loss: 0.5509\n",
      "Epoch 4/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 14s/step - accuracy: 0.8100 - loss: 0.4135 - val_accuracy: 0.6667 - val_loss: 0.6491\n",
      "Epoch 5/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 12s/step - accuracy: 0.8700 - loss: 0.3211 - val_accuracy: 0.6875 - val_loss: 0.5084\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=valid_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training history\n",
    "train_history = model.history.history  # After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss for both training and validation\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(train_history['loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(train_history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09e93d",
   "metadata": {},
   "source": [
    "##### Task 6. Plot the accuracy curves for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec348c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_history = history.history\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(train_history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(train_history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29123d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Function to plot a single image and its prediction\n",
    "def plot_image_with_title(image, model, true_label, predicted_label, class_names):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Convert labels from one-hot to class indices if needed, but for binary labels it's just 0 or 1\n",
    "    true_label_name = class_names[true_label]  # Labels are already in class indices\n",
    "    pred_label_name = class_names[predicted_label]  # Predictions are 0 or 1\n",
    "\n",
    "    plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to test the model with images from the test set\n",
    "def test_model_on_image(test_generator, model, index_to_plot=0):\n",
    "    # Get a batch of images and labels from the test generator\n",
    "    test_images, test_labels = next(test_generator)\n",
    "\n",
    "    # Make predictions on the batch\n",
    "    predictions = model.predict(test_images)\n",
    "\n",
    "    # In binary classification, predictions are probabilities (float). Convert to binary (0 or 1)\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Get the class indices from the test generator and invert them to get class names\n",
    "    class_indices = test_generator.class_indices\n",
    "    class_names = {v: k for k, v in class_indices.items()}  # Invert the dictionary\n",
    "\n",
    "    # Specify the image to display based on the index\n",
    "    image_to_plot = test_images[index_to_plot]\n",
    "    true_label = test_labels[index_to_plot]\n",
    "    predicted_label = predicted_classes[index_to_plot]\n",
    "\n",
    "    # Plot the selected image with its true and predicted labels\n",
    "    plot_image_with_title(image=image_to_plot, model=model, true_label=true_label, predicted_label=predicted_label, class_names=class_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002aa432",
   "metadata": {},
   "source": [
    "##### Task 7. Visualizing the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_plot = 1\n",
    "\n",
    "test_model_on_image(\n",
    "    test_generator,   # test_data_generator\n",
    "    model,            # trained model\n",
    "    index_to_plot=index_to_plot\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3479293",
   "metadata": {},
   "source": [
    "## Part 2. Image captioning and Summarization using BLIP Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d50799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the required libraries\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pretrained BLIP processor and model:\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlipCaptionSummaryLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, processor, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the custom Keras layer with the BLIP processor and model.\n",
    "\n",
    "        Args:\n",
    "            processor: The BLIP processor for preparing inputs for the model.\n",
    "            model: The BLIP model for generating captions or summaries.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, image_path, task):\n",
    "        # Use tf.py_function to run the custom image processing and text generation\n",
    "        return tf.py_function(self.process_image, [image_path, task], tf.string)\n",
    "\n",
    "    def process_image(self, image_path, task):\n",
    "        \"\"\"\n",
    "        Perform image loading, preprocessing, and text generation.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the image file as a string.\n",
    "            task: The type of task (\"caption\" or \"summary\").\n",
    "\n",
    "        Returns:\n",
    "            The generated caption or summary as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode the image path from the TensorFlow tensor to a Python string\n",
    "            image_path_str = image_path.numpy().decode(\"utf-8\")\n",
    "\n",
    "            # Open the image using PIL and convert it to RGB format\n",
    "            image = Image.open(image_path_str).convert(\"RGB\")\n",
    "\n",
    "            # Set the appropriate prompt based on the task\n",
    "            if task.numpy().decode(\"utf-8\") == \"caption\":\n",
    "                prompt = \"This is a picture of\"  # Modify prompt for more natural output\n",
    "            else:\n",
    "                prompt = \"This is a detailed photo showing\"  # Modify for summary\n",
    "\n",
    "            # Prepare inputs for the BLIP model\n",
    "            inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate text output using the BLIP model\n",
    "            output = self.model.generate(**inputs)\n",
    "\n",
    "            # Decode the output into a readable string\n",
    "            result = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            # Handle errors during image processing or text generation\n",
    "            print(f\"Error: {e}\")\n",
    "            return \"Error processing image\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13868452",
   "metadata": {},
   "source": [
    "##### Task 8. Implement a Helper Function to Use the Custom Keras Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to use the custom Keras layer\n",
    "def generate_text(image_path, task):\n",
    "    # Create an instance of the custom Keras layer using the pretrained BLIP processor and model\n",
    "    blip_layer = BlipCaptionSummaryLayer(processor, model)\n",
    "\n",
    "    # Call the layer with the provided inputs\n",
    "    return blip_layer([image_path, task])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a984b22",
   "metadata": {},
   "source": [
    "##### Generating Captions and Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d59542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to an example image \n",
    "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/144_10_JPG_jpg.rf.4d008cc33e217c1606b76585469d626b.jpg\")  # actual path of image\n",
    "\n",
    "# Generate a caption for the image\n",
    "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
    "# Decode and print the generated caption\n",
    "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# Generate a summary for the image\n",
    "summary = generate_text(image_path, tf.constant(\"summary\"))\n",
    "# Decode and print the generated summary\n",
    "print(\"Summary:\", summary.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff1cce",
   "metadata": {},
   "source": [
    "##### Task 9. Generate a caption for an image using the using BLIP pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the following image to display Caption and Summary for Task 9 and 10\n",
    "# URL of the image\n",
    "image_url = \"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\"\n",
    "# Load and display the image\n",
    "img = plt.imread(image_url)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\")  # actual path of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The given image path (already provided in the notebook)\n",
    "image_path = \"/content/sample_image.jpg\"   # <-- Replace only if instructed\n",
    "\n",
    "# Convert image path and task into tensors\n",
    "import tensorflow as tf\n",
    "\n",
    "image_tensor = tf.constant(image_path)\n",
    "task_tensor = tf.constant(\"caption\")\n",
    "\n",
    "# Generate caption using your helper function\n",
    "caption_output = generate_text(image_tensor, task_tensor)\n",
    "\n",
    "# Display the caption\n",
    "caption_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e4ae3",
   "metadata": {},
   "source": [
    "##### Task 10. Generate a summary of an image using BLIP pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\")  # actual path of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The given image path (already provided in the notebook)\n",
    "image_path = \"/content/sample_image.jpg\"   # <-- Keep as provided unless told otherwise\n",
    "\n",
    "# Convert image path and task into tensors\n",
    "import tensorflow as tf\n",
    "\n",
    "image_tensor = tf.constant(image_path)\n",
    "task_tensor = tf.constant(\"summary\")   # <-- Task changed to 'summary'\n",
    "\n",
    "# Generate summary using the helper function\n",
    "summary_output = generate_text(image_tensor, task_tensor)\n",
    "\n",
    "# Display the summary\n",
    "summary_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
